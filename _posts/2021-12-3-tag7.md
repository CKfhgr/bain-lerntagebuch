---
title: Tag 7 - Daten mit OpenRefine bereinigen
date: 2021-12-05
---

Im heutigen Block lernten wir das Programm "OpenRefine" kennen. OpenRefine basiert auf Java und eignet sich dafür um grössere Mengen an Daten einfach zu bereinigen
und anzupassen. Ähnlich wie man es von Excel kennt, kann man in OpenRefine Daten Filtern, Werte ändern, sortieren, schnelle Korrektur vornehmen, und und und... 
(Komme gleich mit ein paar Beispielen darauf zurück)

In diesem Zusammenhang mit Daten fiel der Begriff "Messy Data". OpenRefine ist darauf spezialisiert mit "Messy Data" umgehen zu können. Doch was bedeutet das eigentlich?
Die Definition von "Messy Data" hat der Blog "Trifacta" (1) auf den Punkt gebracht: 
Keine Person sollte mit einem neuen Datensatz arbeiten ohne ihn vorher zu bereinigen. Nicht alle Datensätze sind direkt per se unordendlich, sondern können vielfältige kleine
Unstimmigkeiten enthalten, welche bereinigt werden müssen. Dies um eine korrekte Interpretation der Daten zu ermöglichen. Man möchte ja nicht falsche Schlüsse ziehen!
Trifacta erwähnt in diesem Zusammenhang Beispiele für solche "Verschmutzungen"; ein paar davon liste ich hier auf. Ich habe diese mit ein paar Beispiele ergänzt um es besser zu
verstehen)
- fehlende Daten. *Beispiel: leere Inhalt bei Spalte Sprache. Auf welche Sprache ist nun das Werk? Es muss selbst nachkorrigiert werden*
- unstrukturierte Daten
- Mehrere Variablen in einer Spalte: Inhalte nicht atomar (gleicht eigentlich der ersten Normalform im Kontext Datenbanken). *Beispiel: In Spalte Autor werden drei Autoren für einen Datensatz aufgelistet.*
- Variable in falsche Spalten & vertauschte Spalten. *Beispiel: Vorname in Nachname Spalte gespeichert*
- Zusätzliche Leerzeichen, die nicht benötigt werden
- Inkonsistenz bei Bennenung Variablen. Beispiel: Die Spalte Sprache enthält für englischsprachige Werke mal "EN" und mal "Englisch". Man sollte sich auf eine Sprache einigen.
- ...

So, nun muss ich gestehen, als uns die Software vorgestellt wurde dachte ich die ganze Zeit: "Aber das kann doch Excel auch?". 
Ausserdem hatten wir letztes Semester im Visualisierungskurs "Dynamic User Interfaces" die Programmiersprache "Python" näher angewendet. 
Dort haben wir mittels des Werkzeuges "Pandas" unseren Datensatz zuerst bereinigt und angepasst, damit unsere Auswertung am Ende auch konsistent war. Auch mittels
Pandas sind solche Bereinigungen möglich - eignet sich jedoch nicht für Personen, welche nicht gerne Programmieren. Da ist OpenRefine oder Excel besser, da diese eine
grafische Benutzeroberfläche haben und keine Programmierkenntnisse nötig sind.

Wir installierten OpenRefine auf unserer VM. Wie gewohnt sollten wir wieder unsere Command-Line öffnen, um die entsprechenden Installationsbefehle einzugeben.
Zuerst mussten wir das entsprechende Archiv von https://github.com/OpenRefine/OpenRefine/releases/download/3.5.0/openrefine-linux-3.5.0.tar.gz  herunterladen und entzippen.
Um selbst an einem Beispiel zu testen, durften wir den folgenden Datensatz von Library Carpentry verwenden: https://raw.githubusercontent.com/LibraryCarpentry/lc-open-refine/gh-pages/data/doaj-article-sample.csv 
Das Beispiel war im CSV-Format, jedoch können auch andere Dateiformate von OpenRefine interpretiert werden. Nebst tabellarisch aufgebaute Dateien können auch JSON und XML Formate
hochgeladen werden.

Wir lernten verschiedene Basisfunktionen kennen und machten noch eine kleine "Fingerübung" um uns näher damit ausseinander zu setzen. Da ich gerne solche Anwendungen bediene,
hatte ich eigentlich keine Probleme.
Ich werde hier nicht die Basisfunktionen beschreiben, die ich angewendet habe, das würde den Rahmen dieses Blogposts sprengen. Obwohl ich zu dieser ganzen Thematik noch
viel mehr dazu schreiben könnte ;-). Ich finde aber, dass die Software im Gegensatz zu Excel ohne Vorkenntnisse eingesetzt werden kann. Die Massenüberarbeitung
ist ein wenig einfacher. Bei Excel sind gewisse Funktionen im grossen Menü versteckt und gehen ein wenig unter. OpenRefine ist nur auf das Säubern von Daten spezialisiert. Alle 
Funktionen zielen darauf hin die Daten zu bereinigen.

Die Funktion "Text Facet" ist aber nur praktisch wenn nicht eine unmengen von Daten vorliegen. Weitere Säuberungen können auch noch durchgeführt werden, 
aber alles hat seine Grenzen. Bei grossen Datenbeständen wird es dann bald mal mühsam alle Fehler "auszubürsten".

Die  letzte Unterrichtsstunde musste ich nachholen, da ich noch ein Firmenevent hatte. Es wurde die CSV-Tabelle nach MARCXML exportiert.
Ich habe es dann selbständig am Wochenende nachgeholt und es hatte alles funktioniert. Ich konnte dank der Aufzeichnung, den Ergebnisse aus den durchgeführten Gruppenarbeiten zu
"Reverse Engineering" und den gezeigten Aufgaben "Template ergänzen" und "Validieren mit xmllint" und den Bemerkungen des Dozenten alles genau nachvollziehen.
Das Validiern einer XML-Datei ist sehr wichtig, dafür nutzteb wir die Software "xmllint". Meine Datei wurde am Ende erfolgreich validiert.

(1) https://www.trifacta.com/blog/messy-data/ 








